---
title: "Report"
author: "Liucheng Shi, Zhuohui Liang, Ruwen Zhou, Jiying Han"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(parallel)
library(foreach)
library(doParallel)
library(ggfortify)
library(patchwork)
library(cluster)
library(factoextra) 
library(dendextend) 
library(tidyverse)
library(dbscan)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = F,
  warning = F,
  cache = F
)

theme_set(theme_minimal() + theme(legend.position = "bottom", 
                                  title = element_text(hjust = 0.5)))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(123123)
```

# Introduction
The recent breakthrough in NGS allows us to sequence thousands of RNA simultaneously at individual cell level, which leads to possible insight in heterogeneity in gene expression by dividing cells into subgroups based on their depth of coverage. The objective of this project is to identify the hidden structure in 558 genes using the 716 scRNA sequencing data from breast cancer tumor. We would use clustering method based on GMM model with EM algorithm, in comparison to other methods including hierarchical clustering method.

# Method

## Data Preparation

```{r load_data, echo=F}
sngcll_raw = 
  read_csv("ss.csv") 

sngcll = sngcll_raw %>% 
  janitor::clean_names()
```

```{r standardize_data}
# drop gene that 90% is zero 
drop_gene = 
  sngcll %>% 
  summarise(across(everything(),~sum(.x==0)/n()<0.9)) %>% 
  slice(1) %>% 
  unlist() %>% 
  as.vector()

sngcll_dp0 = 
  sngcll[,drop_gene]
```

It is obvious that the raw single-cell sequencing data is inflated with 0. According to Pierson (2015), PCA using zero-inflated is a less optimal approach in dimension reduction. Followed the adjustment in the paper, we dropped genes with over 90% of zero inputs. 229 genes is used for further PCA and clustering after filtering.

## PCA

```{r pca}
# PCA with scale
sngcll_pca =
  #predict(preProcess(sngcll_dp0,c("center","scale","pca")),sngcll_dp0)
  prcomp( ~ .,
          data = sngcll_dp0,
          tol = sqrt(.Machine$double.eps),
          center = T,
          scale. = T)

summary(sngcll_pca)$importance %>% 
  t() %>% 
  .[seq(1,230,8),] %>% 
  as_tibble(rownames = NA) %>% 
  knitr::kable(digits = 2)

sngcll_pca$rotation[,1:4] %>% t()%>% knitr::kable()
```

```{r}
pca_summary = summary(sngcll_pca)
getpcfeatures = function(i) {
  df = pca_summary$rotation %>% 
    as.data.frame() %>%  mutate(feature = row.names(.)) %>% 
    select(i, feature)
  df = df[abs(df[,1]) >= 0.1,]
  df %>% 
    ggplot(aes(df[,1], reorder(df[,2], df[,1]))) +
    geom_point() +
    xlab(paste("PC",i,sep = "")) +
    ylab("Strong influence")}
pc1 = getpcfeatures(1); pc2 = getpcfeatures(2); pc3 = getpcfeatures(3);
pc4 = getpcfeatures(4); pc5 = getpcfeatures(5); pc6 = getpcfeatures(6)
gridExtra::grid.arrange(pc1,pc2,pc3,pc4,pc5,pc6, nrow = 3)

#eigen = sngcll_pca$sdev^2
#which(eigen >= 1)
fviz_pca_var(sngcll_pca, col.var = "black", repel = TRUE)

tibble(PVE = unname(pca_summary$importance[2,]), PC = seq_along(pca_summary$importance)[1:229]) %>% 
  filter(PVE >= 0.01) %>% 
  ggplot(aes(PC, PVE, group = 1, label = PC)) + 
  geom_bar(stat = "identity", color = "lightgray", fill = "grey") + geom_point() + geom_line() +
  geom_text(nudge_y = -.002) +
  xlab("Principal Components") +
  ylab("Proportion of Variance Explained")
#fviz_eig(sngcll_pca, addlabels = TRUE)
```

Since all depths of coverage and other statistics for gene expression is measured in the same scale, one might have such the intuitive thought that we should not center and scale the single cell expression data. Although both scaled and unscaled methods are implemented in literature, scaling the data would provide us with more details on genome subtypes. Considering gene A required expression level 1000 to be activated and gene B required expression 100, two genes are not comparable and gene A would contribute more to the PCs for bigger variance. Based on the plot, we can see the variance is significantly lower by centering and scaling the expression to 0-1.

The genes contributed the most in the first six PCs are shown below.

## Apply EM algorithm

```{r EM_data}
# EM data preparation
sngcll_pca =
  predict(preProcess(
    sngcll_dp0,
    c("center","scale","pca","center","scale"),
    pcaComp = 20
  ), sngcll_dp0)
```

```{r eval = F}
source("EM.R")

set.seed(123123)
rslt_2 = rerun(1,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt_2,file = "EM.Rdata")

set.seed(123123)
t1 = Sys.time()
rslt = rerun(20,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt,file = "EM_20.Rdata")
t_run = Sys.time() - t1
```



  The model is assumed to have gaussian conditional distribution in each cluster, with parameters $\mu_k$ and $\Sigma_k$ for k in 1,2,3...K. Each observation have probability $p_{ik}$ to be any of the K's clusters, observation is assigned to the cluster with the highest probability. The model is presented as followed:
  
$$\mathbf x_i\sim
\begin{cases}
N(\boldsymbol \mu_1, \Sigma_1), \mbox{with probability }p_1 \\
N(\boldsymbol \mu_2, \Sigma_2), \mbox{with probability }p_2\\
\quad\quad\vdots\quad\quad,\quad\quad \vdots\\
N(\boldsymbol \mu_k, \Sigma_k), \mbox{with probability }p_k\\
\end{cases}$$


  Where i $\in$ 1,2,3...N, the completed likelihood function is:

$$L(\theta; \mathbf x,\mathbf r) = \prod_{i=1}^n \prod_{j=1}^k [p_j f(\mathbf x_i; \boldsymbol \mu_j, \Sigma_j)] ^{r_{i,j}}$$

  The EM algorithm is designed as:
  
\begin{tabular}{c c c}
& &describe\\
\hline\\
1& &initialize model with random $p_j,\mu_j,\Sigma_j$ given k\\
2& while& iteration is less than maximum iteration or objective function not converge\\
&\vline& E step: calculate the conditional probability $p(r_j|X,\mu,\Sigma)$\\
&\vline& M step: calculate and update $\mu,\Sigma,p$ and assign clusters\\
&end&
\end{tabular}

  After optimizing, calculate observed Likelihood L. EM algorithm is optimized by the observed likelihood, but understanding that  assigning each cell to its own cluster would have the model with highest likelihood, but as thus lead to overfilling problem. With above concerns, the AIC loss function($-2(log(L)-n_p)$) is used instead of deviance, where $n_p=K+GK+G^2K+KN$. As rule of thumb, initial cluster's number is set as 2 to 10\cite{system_review}, and final cluster number is determined by model with lowest AIC.
  
# Result


```{r plot}
load("EM.Rdata")

load("EM_20.Rdata")

ggplot(
  rslt %>%
    as_tibble() %>%
    unnest(c(k, obj)) %>%
    filter(obj > 0) %>%
    group_by(k) %>%
    summarise(aic = mean(obj), sd = sd(obj)),
  aes(k, aic)
) +
  geom_point() +
  geom_errorbar(aes(ymin = aic - sd, ymax = aic + sd), width = .3,color = "red" ) +
  scale_x_continuous(limits = c(1.6,10.4),n.breaks = 9)+
  labs(y = "AIC",
       title = "EM clustering (20 runs)")

sngcll_clu_3 = cbind(sngcll_pca,rslt_2[which(rslt_2[,"k"]==3),"cluster"])

pc2 = ggplot(sngcll_clu_3,aes(PC1,PC2,color = cluster))+geom_jitter(alpha = 0.8)

pc3 = ggplot(sngcll_clu_3,aes(PC1,PC3,color = cluster))+geom_jitter(alpha = 0.8)

pc4 = ggplot(sngcll_clu_3,aes(PC2,PC3,color = cluster))+geom_jitter(alpha = 0.8)

pc2 + pc3+pc4 +plot_spacer()+ plot_layout(nrow = 2,ncol = 2, guides = "collect")
```

  The number of cluster is selected based on 20 runs of EM algorithm, and it shows that clustering with 3 clusters has the lowest average AIC. With each cell assigned to cluster, PC complonent plot displays a well separated cluster pattern.

## Gene-expression signatures

```{r}
load("EM_PC20.Rdata")

X_tr = model.matrix(cluster~.,
                    cbind(sngcll_dp0,rslt_2[which(rslt_2[,"k"]==3),"cluster"]))[,-1]

Y_tr= rslt_2[which(rslt_2[,"k"]==3),"cluster"] %>% 
  unlist() 
```

```{r svm_rfd}
set.seed(123123)

Y_tr= rslt_2[which(rslt_2[,"k"]==3),"cluster"] %>% 
  unlist() %>% as.character() %>% str_c("cluster_",.)

cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

rfec = 
  rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   number = 5,
                   verbose = FALSE,
             returnResamp = "final")

svm_12 = rfe(
  X_tr[Y_tr!="cluster_3",],
  Y_tr[Y_tr!="cluster_3"] %>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

svm_23 = rfe(
  X_tr[Y_tr!="cluster_1",],
  Y_tr[Y_tr!="cluster_1"] %>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

svm_13 = rfe(
  X_tr[Y_tr!="cluster_2",],
  Y_tr[Y_tr!="cluster_2"] %>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

svm_all = rfe(
  X_tr,
  Y_tr %>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 10,
  rfeControl= rfec
)

stopCluster(cl)

top_5 = predictors(svm_all) %>% head(5)

plot_data = cbind(sngcll_pca,sngcll_dp0 %>% 
                    mutate_all(function(x) as.factor(ntile(x,5))))

p1 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[1]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[1])

p2 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[2]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[2])

p3 =ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[3]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[3])

p4 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[4]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[4])

p1+p2 + p3 + p4 + plot_layout(nrow =2)+plot_annotation(title = "Top 4 features pecentile plot")
```


```{r multimom, eval = F}
set.seed(123123)

Y_tr= rslt_2[which(rslt_2[,"k"]==3),"cluster"] %>% 
  unlist() 

sngcll_sign = glmnet::cv.glmnet(
  X_tr,
  Y_tr,
  alpha = 1,
  type.measure = "auc",
  family = "multinomial",
  type.family = "ungrouped",
  grouped = F
)

sngcll_sign = 
  coef(sngcll_sign,s = "lambda.1se") %>% 
  do.call(cbind,.) %>% 
  as.matrix() %>% 
  .[-1,]

gene = colnames(sngcll_dp0)

apply(sngcll_sign,2,FUN = function(x) gene[which(x !=0)]) %>% 
  lapply(.,FUN = function(x) str_c(x,collapse = "|| ")) %>% 
  do.call(rbind,.) %>% 
  as_tibble() %>% 
  cbind(cluster = c(1,2,3),.) %>% 
  knitr::kable(caption = "Cluster's signature",
               col.names = c("cluster","signature"),
               align = "c")

ugene = apply(sngcll_sign,2,FUN = function(x) gene[which(x !=0)])
```

```{r Seurat}
seurat.data = as.matrix(scale(sngcll_dp0)) %>% t()
colnames(seurat.data) = str_c("Cell_", seq(1,716))

sngcll_seurat <- CreateSeuratObject(counts = seurat.data, project = "scRNA", min.cells = 1, min.features = 1)
Idents(sngcll_seurat) <- unlist(rslt_2[which(rslt_2[,"k"] == 3),"cluster"])

diff_genes_3 <- FindMarkers(sngcll_seurat, ident.1 = 3, ident.2 = c(1, 2), min.pct = 0.25) #genes differentiate 3 from 1 & 2
diff_genes_2 <- FindMarkers(sngcll_seurat, ident.1 = 2, ident.2 = c(1, 3), min.pct = 0.25)
diff_genes_1 <- FindMarkers(sngcll_seurat, ident.1 = 1, ident.2 = c(2, 3), min.pct = 0.25)
head(diff_genes_3)
head(diff_genes_2)
head(diff_genes_1)
```


  Result from previously session is used as response variable and conduct classification. 


## Analysing Gaussian-Mixture model with Principal Component

  Clustering cell with EM algorithm has several drawbacks. The first of all shared problem with EM algorithm is slow computational time, \cite{very_fast} has compared EM based method with hierarchical methods, and EM method is slower than. The second problem is that EM algorithm's convergence and convergent time are rely on the initialization, a well separated center for initialization provide fast convergence, on the other hand, bad initialization lead to divergence. But the initialization is randomly assigned uniformly,  as a result, convergence is not always guaranteed. The third problem is that gaussian model may not fit RNA sequence expression. \cite{ZIFA} has pointed out that drop-out event leads to zero-inflated data, and gaussian mixture cannot address this problem. Also, \cite{poisson} provide poisson-mixture as a alternative for gaussian mixture.

## Hierarchical clustering

Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. In our project, our purpose is to classify scRNA-seq into different clusters based on their gene expressions and identify potential existence of cell subtypes. We choose agglomerative hierarchical clustering method, which is a bottom-up method. Each object is initially considered as a single-element cluster. At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster. The iteration will not stop until all elements are being classified into one single cluster. This makes the result a tree and can be visualized as a dendrogram.

When measuring the dissimilarity between each pair of observations distance, Euclidean distance has been used. However, when measuring the dissimilarity between two clusters of observations, we have applied different methods.

* Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion.

```{r, include=F}
gene <- read_csv("ss.csv")
gene <- scale(gene)
```

```{r}
# Dissimilarity matrix
d <- dist(gene, method = "euclidean")

# Hierarchical clustering using single Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```
Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 2 groups
sub_grp <- cutree(hc5, k = 2)

# Number of members in each cluster
table(sub_grp)
```

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 2, border = 2:5)
```
Visualize the result in a scatter plot.
```{r}
fviz_cluster(list(data = gene, cluster = sub_grp))
```

Determining Optimal Clusters

* Average Silhouette Method
```{r}
fviz_nbclust(gene, FUN = hcut, method = "silhouette")
```
* Gap Statistic Method
```{r}
gap_stat <- clusGap(gene, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

In order to determine the optimal number of clusters, we applied Average Silhouette Method and Gap Statistic Method. Silhouette Method indicates that two-clusters-model fits our data best, while Gap Statistic Method shows that three-clusters is more suitable. After applying different clusters, we found that when we choose cluster = 3, each cluster has 548, 140, 28 items. When cluster = 2, each cluster has 548, 168 items. 

## Density-based clustering

```{r}
view(sngcll_pca)
kNNdistplot(sngcll_pca1, k = 5)
abline(h = 4)
db = dbscan(sngcll_pca1, eps = 4, minPts = 5)
hullplot(sngcll_pca1, db$cluster)
```

The Density-based clustering method is based on the assumption that points shared similar characteristics would cluster together in a denser format. The method initialize with a core point, and expand the cluster if there are _minPts_ number of neighring points with the radius of _epsilon_.The algorithm would stop when all points are classified as either seeds, borders, or outliers. Two parameters are the minimum number of neighboring points，and the searching radius (epsilon). Using minPts 5, and epsilon 4, we get get 274 noises and 3 cluster containing 546, 5, 8 respectively. Since the PCA is sufficient to preserve distance information, we use PCs to cluster as in part b). Compared with the hierarchical clustering and GMM, density-based clustering perform much worse probably due to the increasing complication in measuring distance for high-dimensional data.

## Conclusion
In order to compare the EM algorithm and Hierarchical clustering method, we apply the silhouette coefficient to evaluate the performance of clustering methods. Since the ground truth labels of clusters are unknown, evaluation must be performed using the model itself. The Silhouette Coefficient s for a single sample is then given as:
$$s = \frac{b-a}{max(a, b)} $$
a: The mean distance between a sample and all other points in the same class.
b: The mean distance between a sample and all other points in the next nearest cluster.


```{r}
plot(silhouette(cutree(hc5, k = 2), dist(gene, method = "euclidean")))
```

```{r}
plot(silhouette(unlist(rslt_2[which(rslt_2[,"k"]==3),"cluster"]), dist(gene, method = "euclidean")))
```

The silhouette coefficient of EM algorithm is 0.12 and silhouette coefficient of hierarchical clustering is 0.21. A higher Silhouette Coefficient score relates to a model with better defined clusters. Therefore, the performance of hierarchical clustering is better than EM algorithm.



