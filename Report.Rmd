---
title: "Group project 2 Report"
author: "Liucheng Shi, Zhuohui Liang, Ruwen Zhou, Jiying Han"
output: 
  pdf_document:
    latex_engine : "xelatex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(parallel)
library(foreach)
library(doParallel)
library(ggfortify)
library(patchwork)
library(cluster)
library(factoextra) 
library(dendextend) 
library(dbscan)
library(tidyverse)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = F,
  warning = F,
  cache = F
)

theme_set(theme_minimal() + theme(legend.position = "bottom", 
                                  title = element_text(hjust = 0.5)))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(123123)
```

# Introduction
The recent breakthrough in NGS allows us to sequence thousands of RNA simultaneously at individual cell level, which leads to possible insight in heterogeneity in gene expression by dividing cells into subgroups based on their depth of coverage. The objective of this project is to identify the hidden structure in 558 genes using the 716 scRNA sequencing data from breast cancer tumor. We would use clustering method based on GMM model with EM algorithm, in comparison to other methods including hierarchical clustering method.

# Method

## Data Preparation

```{r load_data, echo=F}
sngcll_raw = 
  read_csv("ss.csv") 

sngcll = sngcll_raw %>% 
  janitor::clean_names()
```

```{r standardize_data}
# drop gene that 90% is zero 
drop_gene = 
  sngcll %>% 
  summarise(across(everything(),~sum(.x==0)/n()<0.9)) %>% 
  slice(1) %>% 
  unlist() %>% 
  as.vector()

sngcll_dp0 = 
  sngcll[,drop_gene]
```

Raw single-cell sequencing data is inflated with 0. According to Pierson [3], PCA using zero-inflated is a less optimal approach in dimension reduction.Besides, zero-inflation(drop-out) event affecting the converging of EM clustering.  Followed the adjustment in the paper, we dropped genes with over 90% of zero inputs. 229 genes is used for further PCA and clustering after filtering.

## PCA

```{r pca}
# PCA with scale
sngcll_pca =
  #predict(preProcess(sngcll_dp0,c("center","scale","pca")),sngcll_dp0)
  prcomp( ~ .,
          data = sngcll_dp0,
          tol = sqrt(.Machine$double.eps),
          center = T,
          scale. = T)

#summary(sngcll_pca)$importance %>% 
#  t() %>% 
#  .[seq(1,230,8),] %>% 
#  as_tibble(rownames = NA) %>% 
#  knitr::kable(digits = 2)

#sngcll_pca$rotation[,1:4] %>% t()%>% knitr::kable()
```

```{r}
pca_summary = summary(sngcll_pca)
pca_1 = fviz_eig(prcomp(sngcll_dp0), addlabels = TRUE, main = "PVEs before scaling")
pca_2 = fviz_eig(sngcll_pca, addlabels = TRUE, main = "PVEs after scaling")
gridExtra::grid.arrange(pca_1, pca_2, ncol = 2)
getpcfeatures = function(i) {
  df = pca_summary$rotation %>% 
    as.data.frame() %>%  mutate(feature = row.names(.)) %>% 
    select(i, feature)
  df = df[abs(df[,1]) >= 0.1,]
  df %>% 
    ggplot(aes(df[,1], reorder(df[,2], df[,1]))) +
    geom_point() +
    xlab(paste("PC",i,sep = "")) +
    ylab("Strong influence")}
```

Since all depths of coverage and other statistics for gene expression is measured in the same scale, one might have such the intuitive thought that we should not center and scale the single cell expression data. Although both scaled and unscaled methods are implemented in literature, scaling the data would provide us with more details on genome subtypes. Considering gene A required expression level 1000 to be activated and gene B required expression 100, two genes are not comparable and gene A would contribute more to the PCs for bigger variance. Based on the plot, we can see the variance is significantly lower by centering and scaling the expression level.

```{r}
pc1 = getpcfeatures(1); pc2 = getpcfeatures(2); pc3 = getpcfeatures(3);
pc4 = getpcfeatures(4); pc5 = getpcfeatures(5); pc6 = getpcfeatures(6)
gridExtra::grid.arrange(pc1,pc2,pc3,pc4,pc5,pc6, nrow = 3)
fviz_pca_var(sngcll_pca, col.var = "black", repel = TRUE)
```

The genes contributed the most in the first six PCs are shown below, with absolute value of coefficient larger than 0.1. The loading plot also demonstrate how features influence the principal components. Loading ranges from -1 to 1, whereas values close to 1 and -1 shows the feature strongly influence the PC, and values close to 0 shows the feature is weakly influence the PC. RGS5 and HIGB1D strongly influence the first PC, and DBI, FTH1, CASP1, and RBP1 strongly influence the second PC. 

```{r}
tibble(PVE = unname(pca_summary$importance[2,]), PC = seq_along(pca_summary$importance)[1:229]) %>% 
  filter(PVE >= 0.01) %>% 
  ggplot(aes(PC, PVE, group = 1, label = PC)) + 
  geom_bar(stat = "identity", color = "lightgray", fill = "grey") + geom_point() + geom_line() +
  geom_text(nudge_y = -.002) +
  geom_hline(yintercept = 0.01) +
  xlab("Principal Components") +
  ylab("Proportion of Variance Explained")
#eigen = sngcll_pca$sdev^2
#which(eigen >= 1)
```

Based on the proportion of variance explained plot, only the first 20 PCs have the PVE larger than 0.01. All first 20 PCs passed the eigenvalue criterion. Thus, we choose 20 PCs to conduct further clustering.

## EM algorithm

```{r EM_data}
# EM data preparation
sngcll_pca =
  predict(preProcess(
    sngcll_dp0,
    c("center","scale","pca","center","scale"),
    pcaComp = 20
  ), sngcll_dp0)
```

```{r eval = F}
source("EM.R")

set.seed(123123)
rslt_2 = rerun(1,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt_2,file = "EM.Rdata")

set.seed(123123)
t1 = Sys.time()
rslt = rerun(20,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt,file = "EM_20.Rdata")
t_run = Sys.time() - t1
```



  The model is assumed to have gaussian conditional distribution in each cluster, with parameters $\mu_k$ and $\Sigma_k$ for k in 1,2,3...K. Each observation have probability $p_{ik}$ to be any of the K's clusters, observation is assigned to the cluster with the highest probability. The model is presented as followed:
  
$$\mathbf x_i\sim
\begin{cases}
N(\boldsymbol \mu_1, \Sigma_1), \mbox{with probability }p_1 \\
N(\boldsymbol \mu_2, \Sigma_2), \mbox{with probability }p_2\\
\quad\quad\vdots\quad\quad,\quad\quad \vdots\\
N(\boldsymbol \mu_k, \Sigma_k), \mbox{with probability }p_k\\
\end{cases}$$


  Where i $\in$ 1,2,3...N, the completed likelihood function is:

$$L(\theta; \mathbf x,\mathbf r) = \prod_{i=1}^n \prod_{j=1}^k [p_j f(\mathbf x_i; \boldsymbol \mu_j, \Sigma_j)] ^{r_{i,j}}$$

  The EM algorithm is designed as:
  
\begin{tabular}{c c c}
& &describe\\
\hline\\
1& &initialize model with random $p_j,\mu_j,\Sigma_j$ given k\\
2& while& iteration is less than maximum iteration or objective function not converge\\
&\vline& E step: calculate the conditional probability $p(r_j|X,\mu,\Sigma)$\\
&\vline& M step: calculate and update $\mu,\Sigma,p$ and assign clusters\\
&end&
\end{tabular}

  After optimizing, calculate observed Likelihood L. EM algorithm is optimized by the observed likelihood, but understanding that  assigning each cell to its own cluster would have the model with highest likelihood, but as thus lead to overfilling problem. With above concerns, the AIC loss function($-2(log(L)-n_p)$) is used instead of deviance, where $n_p=G*(K+1)+K-1$. As rule of thumb, initial cluster's number is set as 2 to 10\cite{system_review}, and final cluster number is determined by model with lowest AIC.
  
## Hierachical clustering

Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. In our project, our purpose is to classify scRNA-seq into different clusters based on their gene expressions and identify potential existence of cell subtypes. We choose agglomerative hierarchical clustering method, which is a bottom-up method. Each object is initially considered as a single-element cluster. At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster. The iteration will not stop until all elements are being classified into one single cluster. This makes the result a tree and can be visualized as a dendrogram.

## Density method

The Density-based clustering method is based on the assumption that points shared similar characteristics would cluster together in a denser format. The method initialize with a core point, and expand the cluster if there are _minPts_ number of neighboring points with the radius of _epsilon_. The algorithm would stop when all points are classified as either seeds, borders, or outliers. Two parameters are the minimum number of neighboring points，and the searching radius (epsilon). 

  
## Signature selection

  Two methods of gene signature selection is performed and compared. Recursively Feature Elimination Support Vector Machine(SVM-RFE) is classification method based on support vectors, and in each iteration, the predictors with smallest importance are removed until the specified subset is reached. Based on cross-validation method, the subset with best loss function, in our case-Accuracy, is selected. Another method is used is Wilcoxson test with Berforroni p-value adjustment, each cluster's differential expressions are evaluated based on rank based method. Not significant genes are removed and re-evaluated with smaller subset.
  
# Result


```{r plot}
load("EM_PC20.Rdata")

ggplot(
  rslt_2 %>%
    as_tibble() %>%
    unnest(c(k, obj)) %>%
    group_by(k) %>%
    summarise(aic = mean(obj), sd = sd(obj)),
  aes(k, aic)
) +
  geom_path() +
  #geom_errorbar(aes(ymin = aic - sd, ymax = aic + sd), width = .3,color = "red" ) +
  scale_x_continuous(limits = c(1.6,10.4),n.breaks = 9)+
  labs(y = "AIC",
       title = "EM clustering")

sngcll_clu_3 = cbind(sngcll_pca,rslt_2[which(rslt_2[,"k"]==4),"cluster"])

pc2 = ggplot(sngcll_clu_3,aes(PC1,PC2,color = cluster))+geom_point(alpha = 0.8)

pc3 = ggplot(sngcll_clu_3,aes(PC1,PC3,color = cluster))+geom_point(alpha = 0.8)

pc4 = ggplot(sngcll_clu_3,aes(PC1,PC4,color = cluster))+geom_point(alpha = 0.8)

pc5 = ggplot(sngcll_clu_3,aes(PC1,PC5,color = cluster))+geom_point(alpha = 0.8)

pc2 + pc3+pc4 +pc5+ plot_layout(nrow = 2,ncol = 2, guides = "collect")
```

  The number of cluster is selected based on lowest AIC of EM algorithm, and it shows that clustering with 4 clusters has the lowest average AIC. Based on the result of single run, in the case of having 4 clusters, the proportion of cell in each cluster is `r rslt_2[which(rslt_2[,"k"]==4),"p"]$p`. With each cell assigned to cluster, PC component plot displays a well separated cluster pattern.


## Density-based clustering

```{r}
kNNdistplot(scale(sngcll), k = 5)
db = dbscan(scale(sngcll), eps = 25, minPts = 5)
hullplot(scale(sngcll), db$cluster)
```

The KNN distance plot have red-flagged the incapability of clustering. Using minPts 5, and epsilon 25, we get the result of 1 cluster containing 581 points and 135 noises. Density-based clustering fail to capture the underlying genome subtypes possible due to the increasing complication in measuring distance between genes for high-dimensional data. Other clustering methods with greater flexibility should be implemented.


## Hierarchical clustering

When measuring the dissimilarity between each pair of observations distance, Euclidean distance has been used. However, when measuring the dissimilarity between two clusters of observations, we have applied different methods.

* Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion.

```{r, include=F}
gene <- read_csv("ss.csv")
gene <- scale(gene)
```

```{r hclust,cache=T}
# Dissimilarity matrix
d <- dist(gene, method = "euclidean")

# Hierarchical clustering using single Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 2 groups
sub_grp <- cutree(hc5, k = 2)

# Number of members in each cluster
table(sub_grp)
```

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 2, border = 2:5)
```

Visualize the result in a scatter plot.

```{r}
fviz_cluster(list(data = gene, cluster = sub_grp))
```

Determining Optimal Clusters

* Average Silhouette Method

```{r}
fviz_nbclust(gene, FUN = hcut, method = "silhouette")
```

* Gap Statistic Method

```{r gap,cache=T}
gap_stat <- clusGap(gene, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

In order to determine the optimal number of clusters, we applied Average Silhouette Method and Gap Statistic Method. Silhouette Method indicates that two-clusters-model fits our data best, while Gap Statistic Method shows that three-clusters is more suitable. After applying different clusters, we found that when we choose cluster = 3, each cluster has 548, 140, 28 items. When cluster = 2, each cluster has 548, 168 items. 

## Comparison

In order to compare the EM algorithm and Hierarchical clustering method, we apply the silhouette coefficient to evaluate the performance of clustering methods. Since the ground truth labels of clusters are unknown, evaluation must be performed using the model itself. The Silhouette Coefficient s for a single sample is then given as:
$$s = \frac{b-a}{max(a, b)} $$
a: The mean distance between a sample and all other points in the same class.
b: The mean distance between a sample and all other points in the next nearest cluster.


```{r}
plot(silhouette(cutree(hc5, k = 2), dist(gene, method = "euclidean")))
```

```{r}
plot(silhouette(unlist(rslt_2[which(rslt_2[,"k"]==4),"cluster"]), dist(gene, method = "euclidean")))
```

The silhouette coefficient of EM algorithm is 0.08 and silhouette coefficient of hierarchical clustering is 0.21. A higher Silhouette Coefficient score relates to a model with better defined clusters. Therefore, the performance of hierarchical clustering is better than EM algorithm.

## Gene-expression signatures
  
### SVM-RFE
  
```{r}  
load("EM_PC20.Rdata")

X_tr = model.matrix(cluster~.,
                    cbind(sngcll_dp0,rslt_2[which(rslt_2[,"k"]==4),"cluster"]))[,-1]

Y_tr= rslt_2[which(rslt_2[,"k"]==4),"cluster"] %>% 
  unlist() 
```

```{r svm_rfd,cache=T}
set.seed(123123)

Y_tr= rslt_2[which(rslt_2[,"k"]==4),"cluster"] %>% 
  unlist() %>% as.character() %>% str_c("cluster_",.)

cl = parallel::makePSOCKcluster(5)
doParallel::registerDoParallel(cl)

rfec = 
  rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   number = 5,
                   verbose = FALSE,
             returnResamp = "final")

Y_tr1 = Y_tr
Y_tr1[Y_tr!="cluster_1"] = "other"

svm_1 = rfe(
  X_tr,
  Y_tr1%>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

Y_tr2 = Y_tr
Y_tr2[Y_tr!="cluster_2"] = "other"

svm_2 = rfe(
  X_tr,
  Y_tr2%>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

Y_tr3 = Y_tr
Y_tr3[Y_tr!="cluster_3"] = "other"

svm_3 = rfe(
  X_tr,
  Y_tr3%>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

Y_tr4 = Y_tr
Y_tr4[Y_tr!="cluster_4"] = "other"

svm_4 = rfe(
  X_tr,
  Y_tr4%>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 5,
  rfeControl= rfec
)

svm_all = rfe(
  X_tr,
  Y_tr %>% as.factor(),
  sizes = c(5:10,20,50,80,100,125),
  preProcess = c("center"),
  metric = "Accuracy",
  tuneLenght = 10,
  rfeControl= rfec
)

stopCluster(cl)


tibble(cluster = 1:4,
       number_of_signature = c(svm_1$bestSubset,svm_2$bestSubset,svm_3$bestSubset,svm_4$bestSubset),
       top_signature = list(svm_1$optVariables[1:5],svm_2$optVariables[1:5],svm_3$optVariables[1:5],svm_4$optVariables[1:5]) %>% lapply(.,function(x) str_c(x,collapse = "||")) %>% unlist) %>% 
  knitr::kable(caption = "SVM-RFE")

top_5 = predictors(svm_all) %>% head(5)

plot_data = cbind(sngcll_pca,sngcll_dp0 %>% 
                    mutate_all(function(x) as.factor(ntile(x,5))))

p1 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[1]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[1])

p2 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[2]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[2])

p3 =ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[3]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[3])

p4 = ggplot(plot_data,aes_string(x = "PC1", y = "PC2", color = top_5[4]))+
  geom_jitter(alpha = 0.5)+
  labs(colour = top_5[4])

p1+p2 + p3 + p4 + plot_layout(nrow =2)+plot_annotation(title = "Top 4 features pecentile plot")
```


  SVM-RFE method choosing 20 to full genes subset as signature, indicating incapable to reduce features. For each cluster, SVM selected and rank the importance of each gene, the first five gene signature for cluster 1 is `r svm_1$optVariables[1:5]`, `r svm_2$optVariables[1:5]` for cluster 2, `r svm_3$optVariables[1:5]` for cluster 3 and , `r svm_4$optVariables[1:5]` for cluster 4. The number of gene signatures chosen to classify all clusters is 100, which its top 4 signature are plotted against PC component in quantile scale. According to the plot, these gene display similiar distribution to the the clusters.



### Seurat

```{r Seurat}
library(Seurat)
seurat.data = as.matrix(scale(sngcll_dp0)) %>% t()
colnames(seurat.data) = str_c("Cell_", seq(1,716))

sngcll_seurat <- CreateSeuratObject(counts = seurat.data, project = "scRNA", min.cells = 1, min.features = 1)
Idents(sngcll_seurat) <- unlist(rslt_2[which(rslt_2[,"k"] == 4),"cluster"])

diff_genes_4 <- FindMarkers(sngcll_seurat, ident.1 = 4, ident.2 = c(1, 2, 3), min.pct = 0.25) #genes differentiate 3 from 1 & 2

diff_genes_3 <- FindMarkers(sngcll_seurat, ident.1 = 3, ident.2 = c(1, 2, 4), min.pct = 0.25) #genes differentiate 3 from 1 & 2
diff_genes_2 <- FindMarkers(sngcll_seurat, ident.1 = 2, ident.2 = c(1, 3, 4), min.pct = 0.25)
diff_genes_1 <- FindMarkers(sngcll_seurat, ident.1 = 1, ident.2 = c(2, 3, 4), min.pct = 0.25)
idff_genes = FindAllMarkers(sngcll_seurat,min.pct = 0.25)
#head(diff_genes_3)
#head(diff_genes_2)
#head(diff_genes_1)

tibble(cluster_1 = head(diff_genes_1) %>% rownames(), 
       cluster_2 = head(diff_genes_2) %>% rownames(),
       cluster_3 = head(diff_genes_3) %>% rownames(),
      cluster_4 = head(diff_genes_4) %>% rownames()) %>% 
  knitr::kable(caption = "Top 6 genes to differentiate the clusters")

tibble(cluster = 1:4,
       number_of_signature =
         c(nrow(diff_genes_1),nrow(diff_genes_2),nrow(diff_genes_3),nrow(diff_genes_4))) %>% 
  knitr::kable(caption = "Number of signature")

VlnPlot(sngcll_seurat, features = c("timp1","higd1b"))
```
  
<<<<<<< HEAD
  Using the package Seurat [4], we are able to test the difference in gene expression across cluster, and select the gene signatures which shows significant different patterns among clusters by conducting Wilcoxon rank-sum test. No assumption is assigned using the non-parametric test such that it is validated to find gene signatures. Top 6 gene signatures is detected and shown above, in which each genes is sufficient to differentiate the clusters with significant different distribution for each cluster. Based on the violin plot, the gene higd1b in cluster 2 obviously differed with the distribution of expression level in other three clusters. Same for the gene timp1 in cluster 1.  
  Comparing signature of seurat wilcoxon rank test and svm-rfe method, Wilcoxon rank test is better in terms of variation of numbers of signature, but the number of signatures are too high. Whereas SVM-RFE successfully selected fewer signature in some cases and maintain high predictability. Top 5 signatures selected are also varies in two methods.


## Analysing Gaussian-Mixture model with Principal Component

  Clustering cell with EM algorithm has several drawbacks. The first of all shared problem with EM algorithm is slow computational time, \cite{very_fast} has compared EM based method with hierarchical methods, and EM method is slower than. The second problem is that EM algorithm's convergence and convergent time are rely on the initialization, a well separated center for initialization provide fast convergence, on the other hand, bad initialization lead to divergence. But the initialization is randomly assigned uniformly,  as a result, convergence is not always guaranteed. The third problem is that gaussian model may not fit RNA sequence expression. \cite{ZIFA} has pointed out that drop-out event leads to zero-inflated data, and gaussian mixture cannot address this problem. Also, \cite{poisson} provide poisson-mixture as a alternative for gaussian mixture.

## Density-based clustering

```{r}
kNNdistplot(scale(sngcll), k = 5)
db = dbscan(scale(sngcll), eps = 25, minPts = 5)
hullplot(scale(sngcll), db$cluster)
```

The Density-based clustering method is based on the assumption that points shared similar characteristics would cluster together in a denser format. The method initialize with a core point, and expand the cluster if there are _minPts_ number of neighboring points with the radius of _epsilon_. The algorithm would stop when all points are classified as either seeds, borders, or outliers. Two parameters are the minimum number of neighboring points，and the searching radius (epsilon). The KNN distance plot have red-flagged the incapability of clustering. Using minPts 5, and epsilon 25, we get the result of 1 cluster containing 581 points and 135 noises. Density-based clustering fail to capture the underlying genome subtypes possible due to the increasing complication in measuring distance between genes for high-dimensional data. Other clustering methods with greater flexibility should be implemented.


## Hierarchical clustering

Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. In our project, our purpose is to classify scRNA-seq into different clusters based on their gene expressions and identify potential existence of cell subtypes. We choose agglomerative hierarchical clustering method, which is a bottom-up method. Each object is initially considered as a single-element cluster. At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster. The iteration will not stop until all elements are being classified into one single cluster. This makes the result a tree and can be visualized as a dendrogram.

When measuring the dissimilarity between each pair of observations distance, Euclidean distance has been used. However, when measuring the dissimilarity between two clusters of observations, we have applied different methods.

* Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion.

```{r, include=F}
gene <- read_csv("ss.csv")
gene <- scale(gene)
```

```{r}
# Dissimilarity matrix
d <- dist(gene, method = "euclidean")

# Hierarchical clustering using single Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```
* Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 2 groups
sub_grp <- cutree(hc5, k = 2)

# Number of members in each cluster
table(sub_grp)
```

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 2, border = 2:5)
```
Visualize the result in a scatter plot.
```{r}
fviz_cluster(list(data = gene, cluster = sub_grp))
```

Determining Optimal Clusters

* Average Silhouette Method
```{r}
fviz_nbclust(gene, FUN = hcut, method = "silhouette")
```
* Gap Statistic Method
```{r}
gap_stat <- clusGap(gene, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

In order to determine the optimal number of clusters, we applied Average Silhouette Method and Gap Statistic Method. Silhouette Method indicates that two-clusters-model fits our data best, while Gap Statistic Method shows that three-clusters is more suitable. After applying different clusters, we found that when we choose cluster = 3, each cluster has 548, 140, 28 items respectively. When cluster = 2, each cluster has 548, 168 items respectively. 


# Conclusion

In this analysis, we use Gaussian-Mixture model with Principal Component Analysis to explore our dataset. Gaussian mixture model performs soft classification, which means that it can gives us the probability that a given data point belongs to each of the possible clusters. Besides, since our data takes on different shape, it’s better to use Gaussian. Principal component analysis (PCA) is an essential method for analyzing single-cell RNA-seq (scRNA-seq) datasets, but for large-scale scRNA-seq datasets, since PCA algorithms and implementations load all elements of the data matrix into the memory space, which means that method computation time is long and consumes large amounts of memory. Apart from that, the workflow of PCA is redundant and repeated, which can be replaced by fast PCA algorithms, like “Julia” package. Finally, as for EM algorithm, clustering cell with EM algorithm has several drawbacks. The first of all shared problem with EM algorithm is slow computational time, Peijie Lin etc. has compared EM based method with hierarchical methods, and EM method is slower. The second problem is that EM algorithm's convergence and convergent time are rely on the initialization, a well separated center for initialization provide fast convergence, on the other hand, bad initialization lead to divergence. But the initialization is randomly assigned uniformly, as a result, convergence is not always guaranteed. The third problem is that gaussian model may not fit RNA sequence expression. Emma Pierson has pointed out that drop-out event leads to zero-inflated data, and gaussian mixture cannot address this problem. Also, poisson provides poisson-mixture as a alternative for gaussian mixture. Another problem with model-based method is over-estimating. Model-based method has tendency to over-estimate number of cluster, for example, AIC method do not penalize much when K is large, leading to potential of over-estimating.

  On the other hand, graphical method, hierarchical and k-means method do not have strong model assumption as EM method. According to Angelo Duò ect., Graphical method `Seurat` has the fastest run time, followed by biracial method. Model based method is just faster than k-means methods. 

## Reference

[1]"ZIFA: Dimensionality reduction for zero-inflated single-cell gene expression analysis" by Emma Pierson and Christopher Yau

[2]"Gaussian Mixture Models Clustering Algorithm Explained" from
<https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e>

[3]"Benchmarking principal component analysis for large-scale single-cell RNA-sequencing" by Koki Tsuyuzaki1etc.

[4]"CIDR: Ultrafast and accurate clustering through imputation for single-cell RNA-seq data" by Peijie Lin, Michael Troup and Joshua W. K. Ho.

[5]"A systematic performance evaluation of clustering methods for single-cell RNA-seq data [version 3; peer review: 2 approved]" by Angelo Duò, Mark D. Robinson, Charlotte Soneson 

[6]"Sparse graphical models for exploring gene expression data" by Adrian Dobra, Chris Hans, Beatrix Jones, Joseph R. Nevins, Guang Yao, and MikeWest

[7]"pcaReduce: hierarchical clustering of single cell transcriptional profiles" by Justina Žurauskiene and Christopher Yau

[8]"Graphical models for zero-inflated single cell gene expression" by Andrew Mcdavid, Raphael Gottardo, Noah Simon and Mathias Drton

[9]"Supplementary Information for Spatial reconstruction of single-cell gene expression" by Rahul Satija and Jeffrey A. Farrell, David Gennert, Alexander F. Schier, and Aviv Regev

[10]"Model-based clustering for RNA-seq data" by Yaqing Si1, Peng Liu2, Pinghua Li and Thomas P. Brutnell

[11]"Comparing multiple RNA secondary structures using tree comparisons" by Bruce A.Shapiro and Kaizhong Zhang

[12] "Integrated analysis of multimodal single-cell data" by Hao and Hao et al


