---
title: "Report"
author: "Liucheng Shi, Zhuohui Liang, Ruwen Zhou, Jiying Han"
date: "3/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(caret)
library(parallel)
library(foreach)
library(ggfortify)
library(patchwork)
library(cluster)
library(factoextra) 
library(dendextend) 
library(tidyverse)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = F,
  warning = F,
  cache = F
)

theme_set(theme_minimal() + theme(legend.position = "bottom", 
                                  title = element_text(hjust = 0.5)))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(123123)
```

# Introduction
The recent breakthrough in NGS allows us to sequence thousands of RNA simultaneously at individual cell level, which leads to possible insight in heterogeneity in gene expression by dividing cells into subgroups based on their depth of coverage. The objective of this project is to identify the hidden structure in 558 genes using the 716 scRNA sequencing data from breast cancer tumor. We would use clustering method based on GMM model with EM algorithm, in comparison to other methods including hierarchical clustering method.

# Method

## Data Preparation

```{r load_data, echo=F}
sngcll_raw = 
  read_csv("ss.csv") 

sngcll = sngcll_raw %>% 
  janitor::clean_names()
```

```{r standardize_data}
# drop gene that 90% is zero 
drop_gene = 
  sngcll %>% 
  summarise(across(everything(),~sum(.x==0)/n()<0.9)) %>% 
  slice(1) %>% 
  unlist() %>% 
  as.vector()

sngcll_dp0 = 
  sngcll[,drop_gene]
```


## PCA

```{r pca}
# PCA with scale
sngcll_pca =
  #predict(preProcess(sngcll_dp0,c("center","scale","pca")),sngcll_dp0)
  prcomp( ~ .,
          data = sngcll_dp0,
          tol = sqrt(.Machine$double.eps),
          center =F,
          scale. = F)

summary(sngcll_pca)$importance %>% 
  t() %>% 
  .[seq(1,230,8),] %>% 
  as_tibble(rownames = NA) %>% 
  knitr::kable(digits = 2)

sngcll_pca$rotation[,1:4] %>% t()%>% knitr::kable()
```

```{r}
pca_summary = summary(sngcll_pca)
getpcfeatures = function(i) {
  df = pca_summary$rotation %>% 
    as.data.frame() %>%  mutate(feature = row.names(.)) %>% 
    select(i, feature)
  df = df[abs(df[,1]) >= 0.1,]
  df %>% 
    ggplot(aes(df[,1], reorder(df[,2], df[,1]))) +
    geom_point() +
    xlab(paste("PC",i,sep = "")) +
    ylab("Strong influence")}
pc1 = getpcfeatures(1); pc2 = getpcfeatures(2); pc3 = getpcfeatures(3);
pc4 = getpcfeatures(4); pc5 = getpcfeatures(5); pc6 = getpcfeatures(6)
gridExtra::grid.arrange(pc1,pc2,pc3,pc4,pc5,pc6, nrow = 3)

#eigen = sngcll_pca$sdev^2
#which(eigen >= 1)

tibble(PVE = unname(pca_summary$importance[2,]), PC = colnames(pca_summary$importance)) %>% 
  filter(PVE >= 0.05) %>% 
  ggplot(aes(PC, PVE, group = 1, label = PC)) + 
  geom_point() + geom_line() +
  geom_text(nudge_y = -.002) +
  xlab("Principal Components") +
  ylab("Proportion of Variance Explained")
##Use dropped genes, we select two PCs if scaled, three PCs if not scaled
```


## Apply EM algorithm

```{r EM_data}
# EM data preparation
sngcll_pca =
  predict(preProcess(
    sngcll_dp0,
    c("pca","center","scale"),
    pcaComp = sum(summary(sngcll_pca)$importance[3, ]<.9)
  ), sngcll_dp0)
```

```{r eval = F}
source("EM.R")

set.seed(123123)
rslt_2 = rerun(1,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt_2,file = "EM.Rdata")

set.seed(123123)
t1 = Sys.time()
rslt = rerun(20,lapply(10:2, FUN = function(x) gaussian_mixture(sngcll_pca,x,method ="AIC")) %>% do.call(rbind,.)) %>% do.call(rbind,.)
save(rslt,file = "EM_20.Rdata")
t_run = Sys.time() - t1
```



  The model is assumed to have gaussian conditional distribution in each cluster, with parameters $\mu_k$ and $\Sigma_k$ for k in 1,2,3...K. Each observation have probability $p_{ik}$ to be any of the K's clusters, observation is assigned to the cluster with the highest probability. The model is presented as followed:
  
$$\mathbf x_i\sim
\begin{cases}
N(\boldsymbol \mu_1, \Sigma_1), \mbox{with probability }p_1 \\
N(\boldsymbol \mu_2, \Sigma_2), \mbox{with probability }p_2\\
\quad\quad\vdots\quad\quad,\quad\quad \vdots\\
N(\boldsymbol \mu_k, \Sigma_k), \mbox{with probability }p_k\\
\end{cases}$$


  Where i $\in$ 1,2,3...N, the completed likelihood function is:

$$L(\theta; \mathbf x,\mathbf r) = \prod_{i=1}^n \prod_{j=1}^k [p_j f(\mathbf x_i; \boldsymbol \mu_j, \Sigma_j)] ^{r_{i,j}}$$

  The EM algorithm is designed as:
  
\begin{tabular}{c c c}
& &describe\\
\hline\\
1& &initialize model with random $p_j,\mu_j,\Sigma_j$ given k\\
2& while& iteration is less than maximum iteration or objective function not converge\\
&\vline& E step: calculate the conditional probability $p(r_j|X,\mu,\Sigma)$\\
&\vline& M step: calculate and update $\mu,\Sigma,p$ and assign clusters\\
&end&
\end{tabular}

  After optimizing, calculate observed Likelihood L. EM algorithm is optimized by the observed likelihood, but understanding that  assigning each cell to its own cluster would have the model with highest likelihood, but as thus lead to overfilling problem. With above concerns, the AIC loss function($-2(log(L)-n_p)$) is used instead of deviance, where $n_p=K+GK+G^2K+KN$. As rule of thumb, initial cluster's number is set as 2 to 10\cite{system_review}, and final cluster number is determined by model with lowest AIC.
  
# Result

## Gene-expression signatures

## Analysing Gaussian-Mixture model with Principal Component

```{r plot}
load("EM.Rdata")

ggplot(as_tibble(rslt_2) %>% unnest(c(k, obj)), aes(k, obj)) +
  geom_path()+
  labs(y = "AIC")

sngcll_clu_3 = cbind(sngcll_pca,rslt_2[which(rslt_2[,"k"]==3),"cluster"])

pc2 = ggplot(sngcll_clu_3,aes(PC1,PC2,color = cluster))+geom_jitter(alpha = 0.8)

pc3 = ggplot(sngcll_clu_3,aes(PC1,PC3,color = cluster))+geom_jitter(alpha = 0.8)

pc4 = ggplot(sngcll_clu_3,aes(PC1,PC4,color = cluster))+geom_jitter(alpha = 0.8)

pc2 + pc3+pc4 +plot_spacer()+ plot_layout(nrow = 2,ncol = 2, guides = "collect")
```



  Clustering cell with EM algorithm has sevaral drawbacks. The first of all shared problem with EM algorithm is slow computational time, \cite{very_fast} has compared EM based method with hierarchical methods, and EM method is slower than. The second problem is that EM algorithm's convergence and convergent time are rely on the initialization, a well separated center for initialization provide fast convergence, on the other hand, bad initialization lead to divergence. But the initialization is randomly assigned uniformly,  as a result, convergence is not always guaranteed. The third problem is that gaussian model may not fit RNA sequence expression. \cite{ZIFA} has pointed out that drop-out event leads to zero-inflated data, and gaussian mixture cannot address this problem. Also, \cite{poisson} provide poisson-mixture as a alternative for gaussian mixture.

## Hierarchical clustering

Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. In our project, our purpose is to classify scRNA-seq into different clusters based on their gene expressions and identify potential existence of cell subtypes. We choose agglomerative hierarchical clustering method, which is a bottom-up method. Each object is initially considered as a single-element cluster. At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster. The iteration will not stop until all elements are being classified into one single cluster. This makes the result a tree and can be visualized as a dendrogram.

When measuring the dissimilarity between each pair of observations distance, Euclidean distance has been used. However, when measuring the dissimilarity between two clusters of observations, we have applied different methods.

* Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion.

```{r}
gene <- read_csv("ss.csv")
gene <- scale(gene)
head(gene)
```

```{r}
# Dissimilarity matrix
d <- dist(gene, method = "euclidean")

# Hierarchical clustering using single Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```
Wardâ€™s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 3)

# Number of members in each cluster
table(sub_grp)
```

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 3, border = 2:5)
```
Visualize the result in a scatter plot.
```{r}
fviz_cluster(list(data = gene, cluster = sub_grp))
```

Determining Optimal Clusters
* Elbow Method
```{r}
fviz_nbclust(gene, FUN = hcut, method = "wss")
```
* Average Silhouette Method
```{r}
fviz_nbclust(gene, FUN = hcut, method = "silhouette")
```
* Gap Statistic Method
```{r}
gap_stat <- clusGap(gene, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

## Conclusion